---
title: "All about the Bayes answers"
subtitle: "Bayes standalone code"
author: "jsg"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y %H:%M')`"
output:
  html_document:
    toc: true
    toc_float: true
    keep_md: true
    self_contained: true
---

Before doing this, review the lecture set slides from 
https://sites.google.com/view/biostats/lessons/intro-to-bayesian-statistics and
the 
**bayes_standalone.R** script in the lecture files folder of the
[CUNY-BioStats github repository](https://github.com/jsgosnell/CUNY-BioStats). 
Make sure you are comfortable with null and alternative hypotheses for all
examples.

Remember you should

* add code chunks by clicking the *Insert Chunk* button on the toolbar or by
pressing *Ctrl+Alt+I* to answer the questions!
* **knit** your file to produce a markdown version that you can see!
* save your work often 
  * **commit** it via git!
  * **push** updates to github
  

1.  Make sure you can describe the main differences between Frequentist, Likelihood, and Bayesian approaches.

*Answer should focus on differences in perception of "data" and tests. Frequentists
consider real world parameters to be absolute but focus on "noise" in our samples
of it (sampling error) so that tests rely on infinite samples (p-value). Bayesians
consider real world parameters probabilistically. They fit a distribution to what
they think the world looks like (prior), collect data to consider how likely it 
would be if that were true, and focus on combining those to update their beliefs
(posterior).

2.  Review the video we watched in class to make sure you understand the Bayesian 
connection. You can also read a related post @ https://brilliant.org/wiki/monty-hall-problem/.
* https://www.youtube.com/watch?v=mhlc7peGlGg

3.  I've shared a script in R that lets you test the Monty Hall idea (like in 
the video!).  It's the chivers_monty_hall_script from the 
[code_examples folder](https://github.com/jsgosnell/CUNY-BioStats/tree/master/code_examples)code_examples  
on github.  For this question, its easiest to just source the main file and see what happens.
When you source a script, it is run in R without showing any console output
(but graphs and objects are still produced!).  Try 
*source("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/code_examples/chivers_monty_hall_script.R")*
, then test out the idea here using the following functions which calculate outcomes
under each strategy.
* monty(strat="stay", print_games=F)
* monty(strat="switch", print_games=F)
* monty(strat="random", print_games=F)

```{r}
source("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/code_examples/chivers_monty_hall_script.R")
monty(strat="stay", print_games=F)
monty(strat="switch", print_games=F)
monty(strat="random", print_games=F)
```

4. Setup the Monty Hall problem as probabilities and convince yourself how it works.
You may want to remember to think about prior and new information (likelihoods).

5. Run the frog analysis (14/18 frogs are right-pawed) assuming an “uninformed” 
prior (is this really possible?) and priors that predict frogs are likely to be 
left- or right-handed (look under Bayesian analysis in script for functions such
as triplot and qbeta).  Vary both the relationship among the shape variables 
and the magnitude (weighting) to understand how the prior impacts your posterior.

```{r}
library(LearnBayes)
#even, uniform (uninformed) prior
triplot(prior = c(1,1), data = c(14,4), where = "topleft")
#prior assumes left handed
triplot(prior = c(5,20), data = c(14,4), where = "topleft")
#prior assumes right handed 
triplot(prior = c(20,5), data = c(14,4), where = "topleft")
#less sure right handed
triplot(prior = c(4,2), data = c(14,4), where = "topleft")
```
*This is the "big picture" of Bayesian analysis. We combined our prior beliefs and
data to update our beliefs (the posterior). We sample/describe the posterior for
our answer. Everything else is "how do we do that?".*

6. Using data from Australian athletes (http://www.statsci.org/data/oz/ais.html 
for details), determine if the average male training at the Australian Institute 
of Sport differs in weight from the average Australian male (85.9 kg) using
bootstrapping techniques and a Bayesian approach. For the Bayesian approach, 
compare approaches that give the null more and less weight.

Data at 
```{r}
sport <- read.table("http://www.statsci.org/data/oz/ais.txt", header = T, 
                    stringsAsFactors = T)
```

You can source the bootstrapjsg function using

```{r}
source("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/code_examples/bootstrapjsg.R")
```

*Answer*

```{r}
sport <- read.table("http://www.statsci.org/data/oz/ais.txt", header = T)
source("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/code_examples/bootstrapjsg.R")
bootstrapjsg(data1=sport[sport$Sex == "male", "Wt"], null=85.9)
#to get estimates!
summary(sport[sport$Sex == "male",])
hist(sport[sport$Sex == "male", "Wt"])
```


```{r}
library(BayesFactor)
ttestBF(sport[sport$Sex == "male", "Wt"], mu=85.9)
ttestBF(sport[sport$Sex == "male", "Wt"], mu=85.9, rscale = "ultrawide")
```
*Both answers give a Bayes Factor >1, which is evidence against the null. However, 
support is fairly weak (2.12-3.8, largely anecdotal), and you can see the 
"ultrawide" option gives even more 
weight to the null."*

7. Data on plant heights (in cm) for plants grown with a new and old formulation 
of fertilizer can be found at

https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/fertilizer.csv

Use the data to test the hypothesis that there is no difference in mean plant
heights for the two groups A) Using frequentist methods B) Using Bayesian
approaches.

```{r}
#fertilizer####
fertilizer <- read.csv("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/fertilizer.csv")

t.test(height ~ fertilizer, fertilizer)
library(BayesFactor)
ttestBF(formula = height ~ fertilizer, data = fertilizer)
```
*Using a frequentist t-test (since the data focuses on a continuous response 
variable and differences among 2 groups), we find t~15.559~=3.013, p<.01, so we
reject the null hypothesis of no difference among groups.  We can also we find a
Bayes Factor >1, which is substantial evidence against the null. However, 
support is not as strong as might expect given the very low p-value*

8.Develop a Bayesian model to determine if sepal width (from the iris dataset in
R) differs among populations.

* compare models that parameterize each population as different vs one that only 
examines difference between I. setosa and other species.
  + making a new dummy variable is one way to do this!
  
```{r}
#ANOVA example####
#with stan for all species
library(rstanarm)
bayesian_iris_anova <- stan_aov(Sepal.Width~Species, data = iris, 
                                prior = R2(what = "median", location = 0.5), adapt_delta = 0.9999)

#check sampling outcomes####
launch_shinystan(bayesian_iris_anova)

#check model#### 
#check residuals for patterns
require(ggplot2)
resid = resid(bayesian_iris_anova)
fit = fitted(bayesian_iris_anova)
ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))

#can also look at  posterior predictive checks in shinystan

#does model predict observed data?
#http://www.flutterbys.com.au/stats/tut/tut7.4b.html
y_pred = posterior_predict(bayesian_iris_anova)
#just getting all simulated outcomes into a column
require(tidyr)
newdata = iris[,c("Sepal.Width", "Species")] %>% cbind(t(y_pred)) %>% gather(key = "Rep", value = "Sepal.Width",
                                                                              -"Species":-"Sepal.Width")
require(ggplot2)
ggplot(newdata) + 
  geom_violin(aes(y = Sepal.Width, x = Species, fill = "Model"),
              alpha = 0.5) + 
  geom_violin(data = iris, aes(y = Sepal.Width, x = Species,fill = "Obs"), alpha = 0.5) + 
  geom_point(data = iris, aes(y = Sepal.Width, x= Species), 
             position = position_jitter(width = 0.1, height = 0),
             color = "black")


#analyze posterior####
#have to call certainty interval by dummy variable!
summary(bayesian_iris_anova)

ci95 <- posterior_interval(bayesian_iris_anova, prob = 0.95, pars = "Speciesversicolor")
round(ci95, 2)
ci95 <- posterior_interval(bayesian_iris_anova, prob = 0.95, pars = "Speciesvirginica")
round(ci95, 2)
ci95 <- posterior_interval(bayesian_iris_anova, prob = 0.95, pars = "(Intercept)")
round(ci95, 2)

#make model with virginican and versicolor as same species####

iris$combined.species <- iris$Species
levels(iris$combined.species) <- c("setosa", "combined", "combined")
bayesian_iris_anova_combined <- stan_aov(Sepal.Width~combined.species, data = iris, 
                                prior = R2(what = "median", location = 0.5), adapt_delta = 0.9999)

#check sampling outcomes####
launch_shinystan(bayesian_iris_anova_combined)

#check model#### 
#check residuals for patterns
resid = resid(bayesian_iris_anova_combined)
fit = fitted(bayesian_iris_anova_combined)
ggplot() + geom_point(data = NULL, aes(y = resid, x = fit))

#can also look at  posterior predictive checks in shinystan

#does model predict observed data?
#http://www.flutterbys.com.au/stats/tut/tut7.4b.html
y_pred = posterior_predict(bayesian_iris_anova_combined)
#just getting all simulated outcomes into a column
newdata = iris[,c("Sepal.Width", "combined.species")] %>% cbind(t(y_pred)) %>% gather(key = "Rep", value = "Sepal.Width",
                                                                             -"combined.species":-"Sepal.Width")
ggplot(newdata) + 
  geom_violin(aes(y = Sepal.Width, x = combined.species, fill = "Model"),
              alpha = 0.5) + 
  geom_violin(data = iris, aes(y = Sepal.Width, x = combined.species,fill = "Obs"), alpha = 0.5) + 
  geom_point(data = iris, aes(y = Sepal.Width, x= combined.species), 
             position = position_jitter(width = 0.1, height = 0),
             color = "black")


#analyze posterior####
#have to call certainty interval by dummy variable!
summary(bayesian_iris_anova_combined)

ci95 <- posterior_interval(bayesian_iris_anova_combined, prob = 0.95, pars = "combined.speciescombined")
round(ci95, 2)

ci95 <- posterior_interval(bayesian_iris_anova, prob = 0.95, pars = "(Intercept)")
round(ci95, 2)


#compare models using loo
#leave one out cross validation #not in lecture
loo_bayesian_iris_anova <- loo(bayesian_iris_anova)
print(loo_bayesian_iris_anova)
loo_bayesian_iris_anova_combined <- loo(bayesian_iris_anova_combined)
print(loo_bayesian_iris_anova_combined)
loo_compare(loo_bayesian_iris_anova,loo_bayesian_iris_anova_combined)

#or using an information criterion
waic_bayesian_iris_anova <- waic(bayesian_iris_anova)
print(waic_bayesian_iris_anova)
waic_bayesian_iris_anova_combined <- waic(bayesian_iris_anova_combined)
print(waic_bayesian_iris_anova_combined)
loo_compare(waic_bayesian_iris_anova,waic_bayesian_iris_anova_combined) #model
```
*Sample code provided. Major point is using Shinystan to visually access that
burn-in period is sufficient and chains converger prior to analyzing the posterior.*
