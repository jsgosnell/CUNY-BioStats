---
title: "9. Multiple regression and extensions"
author: "jsg"
date: "9/30/2020"
output: github_document
---
Before doing this, review the **Week 6** lecture set slides from 
https://sites.google.com/view/biostats/bio-7800178002/week-9 and
the  **10_ANCOVA_and_Regression.R**(from line 248) and 
*11_Mixed_models_and_other_extensions.R** script in the lecture files folder of the
[CUNY-BioStats github repository](https://github.com/jsgosnell/CUNY-BioStats). 
Make sure you are comfortable with null and alternative hypotheses for all
examples.

This assignment and the lecture it follows are meant as a starting point for 
these models! You could develop entire courses devoted to these methods, so 
focus on their connections to linear models and when you would use them.

Remember you should

* add code chunks by clicking the *Insert Chunk* button on the toolbar or by
pressing *Ctrl+Alt+I* to answer the questions!
* **knit** your file to produce a markdown version that you can see!
* save your work often 
  * **commit** it via git!
  * **push** updates to github
  

1.  Data on the height, diameter, and volume of cherry trees was collected for
use in developing an optimal model to predict timber volume.  Data is available @ 

http://www.statsci.org/data/general/cherry.txt

Use the data to justify an optimal model.

```{r}
cherry <- read.table("http://www.statsci.org/data/general/cherry.txt",
                     header = T)
head(cherry)

#if only considering main effects (one option)
cherry_full <- lm(Volume ~ Diam + Height, cherry)
plot(cherry_full)
library(car)
Anova(cherry_full, type = "III")
#both are significant, so finished

#could also consider interactions 
cherry_full <- lm(Volume ~ Diam * Height, cherry)
plot(cherry_full)
Anova(cherry_full, type = "III")
summary(cherry_full)
#all significant, so finished
```
*I used multiple regression to consider the impacts of both continuous and 
categorical explanatory variables on timber volume.  I used a top-down approach 
focused on p-values (F tests) in this example.  Both diameter and height (and their
interaction) are significant, so the full model is justified by the data. It 
explains 97.5% of the variation in volume.  AIC methods lead to a similar outcome*

```{r}
library(MASS)
stepAIC(cherry_full)
```

2. Over the course of five years, a professor asked students in his stats class 
to carry out a simple experiment.  Students were asked to measure their pulse 
rate, run for one minute, then measure their pulse rate again.  The students 
also filled out a questionnaire.  Data  include:

Variable | Description
-------  | ----------
Height | Height (cm)
Weight | Weight (kg)
Age    | Age (years)
Gender | Sex (1 = male, 2 = female)
Smokes | Regular smoker? (1 = yes, 2 = no)
Alcohol | Regular drinker? (1 = yes, 2 = no)
Exercise | Frequency of exercise (1 = high, 2 = moderate, 3 = low)
Change | Percent change in pulse (pulse after experiment/pulse before experiment)
Year | Year of class (93 - 98)

Using the available data (available at 

https://docs.google.com/spreadsheets/d/e/2PACX-1vToN77M80enimQglwpFroooLzDtcQMh4qKbOuhbu-eVmU9buczh7nVV1BdI4T_ma-PfWUnQYmq-60RZ/pub?gid=942311716&single=true&output=csv )

determine the optimal subset of explanatory variables that should be used to
predict change pulse rate (Change) (focusing on main effects only, no 
interactions) and explain your choice of methods.  Interpret your results. Make
sure you can explain any changes you needed to make to the dataset or steps you 
used in your analysis.

```{r}
pulse_class_copy <- read.csv("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/pulse_class_copy.csv", stringsAsFactors = T)
pulse_class_copy$Gender <- as.factor(pulse_class_copy$Gender)
pulse_class_copy$Smokes <- as.factor (pulse_class_copy$Smokes)
pulse_class_copy$Alcohol <- as.factor(pulse_class_copy$Alcohol)

require(MuMIn)
pulse_full <- lm(Change ~ ., pulse_class_copy )
pulse_final <- step(pulse_full)
#consider assumptions
plot(pulse_final)
Anova(pulse_final, type = "III")


#or
library(MuMIn)
options(na.action = "na.fail")
auto <- dredge(pulse_full)
write.csv(auto, "dredge_output.csv", row.names = F)
options(na.action = "na.omit")

```
*I used step based approach (which requires nested models) and large search method
above. Using the step approach only height and alcohol usage are retained in the 
final model, which explains 10% of the variation in pulse change. Model assumptions 
are also met. The search method finds the same optimal model but notes many other
models (including a null model) perform similarly well.*

3. In a study considering how the presence of sea stars changed snail growth 
patterns, ~25 snails were grown in containers containing 0,1, or 2 seastars.  
Since non-consumptive effects are often threshold based, these treatments levels
should be considered as groups (not as a continuous variable!).  The data is 
available at

https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/snail_modified_for_class.csv   

FL is the final length of measured snails, and the treatment (coded 1-3) correspond
to  [1=Control (no predators). 2=1 predator treatment,3=2 predator treatment). 

What method would you use to analyze this data and why? Carry out your test, 
stating your null hypothesis, test assumptions, p-value, and interpretation.  
Describe any necessary steps and provide graphics and values as needed.  If 
needed, can you determine which treatments differ from each other?

```{r}
snail <- read.csv("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/snail_modified_for_class.csv")
head(snail)
snail$Treatment <- as.factor(snail$Treatment)
require(plyr)
snail$Treatment_new <- revalue(snail$Treatment, c("1" = "Control", "2" = "Single predator",
                                                  "3" = "Two predators"))

require(lme4)
snail_mm <- lmer(FL ~ Treatment_new + (1|Container), snail)
summary(snail_mm)
plot(snail_mm)
check_mixed_model <- function (model, model_name = NULL) {
  #collection of things you might check for mixed model
  par(mfrow = c(2,3))
  #not sure what this does with mutliple random effects, so stop with 1 for now
  if(length(names(ranef(model))<2)){
    qqnorm(ranef(model, drop = T)[[1]], pch = 19, las = 1, cex = 1.4, main= paste(model_name, 
                                                                                  "\n Random effects Q-Q plot"))
  }
  plot(fitted(model),residuals(model), main = paste(model_name, 
                                                    "\n residuals vs fitted"))
  qqnorm(residuals(model), main =paste(model_name, 
                                       "\nresiduals q-q plot"))
  qqline(residuals(model))
  hist(residuals(model), main = paste(model_name, 
                                      "\nresidual histogram"))
}

check_mixed_model(snail_mm)


require(car)
Anova(snail_mm, type = "III")

require(multcomp)
snail_comparison <- glht(snail_mm, linfct = mcp(Treatment_new = "Tukey"))
summary(snail_comparison)

#graph using Rmisc
library(Rmisc)
library(ggplot2)
graph_output <- summarySE(snail, measurevar = "FL", groupvars = "Treatment_new")
bar_graph_with_error_bars <- ggplot(graph_output, 
                                     aes_string(x="Treatment_new", 
                                                y = "FL")) +
  geom_col() + 
  geom_errorbar(aes(ymin = FL - ci, 
                    ymax = FL + ci))+
  xlab("Treatment")+
  theme(axis.title.x = element_text(face="bold", size=28), 
        axis.title.y = element_text(face="bold", size=28), 
        axis.text.y  = element_text(size=20),
        axis.text.x  = element_text(size=20), 
        legend.text =element_text(size=20),
        legend.title = element_text(size=20, face="bold"),
        plot.title = element_text(hjust = 0.5, face="bold", size=32))+
ylim(c(0, 30))

bar_graph_with_error_bars

```
*Since multiple oysters were measured in each cage, we need to use a random effect
to account for cages. You could also block by cages- it takes up more degrees of 
freedom, but you have plenty here.  Results show a significant differce among 
treatments (Chi^2~2=10.681, p <.01), so I used a Tukey post hoc test to determine 
which groups differed from others while controlling for the family wise error rate.
REsults indicate the presence of a predator impacts length but not the density.*

4. (From OZDasl) The data give the ambient temperature and the number of 
primary O-rings damaged for 23 of the 24 space shuttle launches before the 
launch of the space shuttle Challenger on January 20, 1986. (Challenger was the
25th shuttle. One engine was lost at sea and could not be examined.) Each space
shuttle contains 6 primary O-rings.

Note these are counts. We can analyze this data using a Poisson distribution 
or binomial. Make sure you understand why each one is possible, which one is 
better, and carry out the analysis.  Data is available @ 

http://www.statsci.org/data/general/challenger.txt

```{r}
rings <- read.table("http://www.statsci.org/data/general/challenger.txt", 
                    header = T)
#can do as poisson
rings_poisson <- glm(Damaged ~ Temp, rings, family = "poisson")
summary(rings_poisson)
#note dispersion is ok
require(car)
Anova(rings_poisson, type = "III")
#or binomial (preffered as we can add info (number damaged and not!))
rings_binomial <- glm(cbind(Damaged, 6 - Damaged) ~ Temp, rings, family = "binomial")
summary(rings_binomial)
#note dispersion is ok
Anova(rings_binomial, type = "III")
#compare to lm
rings_lm <- lm(Damaged ~ Temp, rings)
summary(rings_lm)
#note dispersion is ok
Anova(rings_lm, type = "III")
```

*Since these are counts we need to use a glm to model the data. We could use a 
Poisson, but the binomial actually includes more information (like how many did
not fail!). Both models indicate a significant relationship between temperature
and the number or proportion of failed rings. Results are compared to a linear model.*

5. Returning to the whelk length-mass relationship from class, try fitting an 
exponential curve to the data.  As a hint, try

```{r eval=F}
nls(Mass ~ exp(b0 + b1 * Shell.Length), whelk, 
                   start = list(b0 =1, b1=0), na.action = na.omit)
```

Compare this model to those that assume a linear and power relationship.  Data is available @

https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/whelk.csv 

```{r}
whelk <- read.csv("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/whelk.csv")
head(whelk)
summary(whelk)
require(ggplot2)
whelk_plot <- ggplot(whelk, aes_string(x="Shell.Length", y = "Mass")) +
  geom_point(aes_string(colour = "Location")) + 
  theme(axis.title.x = element_text(face="bold", size=28), 
        axis.title.y = element_text(face="bold", size=28), 
        axis.text.y  = element_text(size=20),
        axis.text.x  = element_text(size=20), 
        legend.text =element_text(size=20),
        legend.title = element_text(size=20, face="bold"),
        plot.title = element_text(hjust = 0.5, face="bold", size=32))
whelk_plot
#power fit
whelk_lm <- lm(Mass ~ Shell.Length, whelk, na.action = na.omit)

whelk_power <- nls(Mass ~ b0 * Shell.Length^b1, whelk, 
                   start = list(b0 = 1, b1=3), na.action = na.omit)
whelk_exponential <- nls(Mass ~ exp(b0 + b1 * Shell.Length), whelk, 
                         start = list(b0 =1, b1=0), na.action = na.omit)
library(MuMIn)
AICc(whelk_lm, whelk_power, whelk_exponential)

#plot
whelk_plot + geom_smooth(method = "lm", se = FALSE, size = 1.5, color = "orange")+ 
  geom_smooth(method="nls", 
              # look at whelk_power$call
              formula = y ~ b0 * x^b1, 
              method.args = list(start = list(b0 = 1, 
                                              b1 = 3)), 
              se=FALSE, size = 1.5, color = "blue") +
  geom_smooth(method="nls", 
              # look at whelk_exponential$call
              formula = y ~ exp(b0 + b1 * x), 
              method.args = list(start = list(b0 = 1, 
                                              b1 = 0)), 
              se=FALSE, size = 1.5, color = "green")
```
*We can use the nls model to consider exponential curve to the data. Various fits
may be compared using AIC methods.  In this case it appears that the power fit is 
the best (lowest AIC value).*

6. Going back to the TEAM dataset, remember we found that elevation had no 
impact on carbon storage.  But that was a linear fit. Use a gam (generalized 
additive model) to see if elevation can be related to carbon storage in an 
additive model.  Note we can use the gamm (generalized additive mixed model) 
function in the mgcv package to denote mixed effects.  For example (from help 
file)
```{r eval=F}
b2 <- gamm(y~s(x0)+s(x1)+s(x2),family=poisson,
           data=dat,random=list(fac=~1))
```

Team data is available @ 

https://raw.github.com/jsgosnell/CUNY-BioStats/blob/master/datasets/team_data_no_spaces.csv 

```{r}
require(mgcv)
require(MuMIn) #for AICc
team <- read.csv("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/team_data_no_spaces.csv", stringsAsFactors = T)
elevation_linear <- gam(PlotCarbon.tonnes ~ Elevation, data = team)
elevation_gam <- gam(PlotCarbon.tonnes ~ s(Elevation), data = team)
elevation_gamm <- gamm(PlotCarbon.tonnes ~s(Elevation), random = list(Site.Name = ~ 1), data = team)
AICc(elevation_gam, elevation_gamm, elevation_linear)
```
*A generalized additive model fits a curve to the dataset (spline in this case). 
AIC comparison indicates the gam model with a random effect for site is the best
fit.*

## Tree graph review

7. Kyphosis refers an issue with spinal curvature. Use the kyphosis dataset
built into rpart to build and compare classification trees that minimize 
mis-labeling (default, Gini’s index) and that maximize information gain 
(add parms = list(split = 'information') to your rpart call).  

```{r}
require(rpart)
head(kyphosis)
kyphosis_tree_information <- rpart(Kyphosis ~ ., data = kyphosis, 
                                   parms = list(split = 'information'))
plot(kyphosis_tree_information)
text(kyphosis_tree_information)

kyphosis_tree_gini <- rpart(Kyphosis ~ ., data = kyphosis)
plot(kyphosis_tree_gini)
text(kyphosis_tree_gini)
```




