---
title: "2022 Fall 78002 Final (30 points total)"
subtitle:  "Thanks for a good semester! Good luck!"
author: "jsg answers"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y %H:%M')`"
output:
  html_document:
    toc: true
    toc_float: true
    keep_md: true
    self_contained: true
    toc_depth: 6
---

#Read this first.

These answers are meant for review **and** learning. In using real data I often
find methods we did not cover in class. They are demonstrated here but not required
for credit on exams.  Also, my final answers may not match what was in the 
inspiring/provided papers. This may be due to missing data or mistakes (or my part
most likely!), but it illustrates the benefits of sharing code!

# Amphipods!

Following up on #1 - 3 from class. As a reminder/intro:

>A colleague studying reef organisms wanted to consider if the willingness of 
juvenile amphipods to disperse to a novel algal habitat is impacted by the current 
habitat they inhabit or presence of adult amphipods on the novel habitat.  In a lab
study, they stocked portions of aquariums with pieces of algal (the novel piece of 
algae, below).  Half the algae pieces had adult residents (4 total; the species 
builds tubes to live on algae) while the other half had no adult residents.  
Juvenile amphods (16 total) were then added to the aquarium. Juveniles were placed on 
one of 3 habitats (natural substrate, artificial substrate, or bare (no substrate). 
The experiment had 6  replicates for each habitat-adult combination. After 
24 hours your colleague counted the number of juveniles that had moved to the
novel piece of algae.  

>Their PI helped them design the study, but they lost the notes from the initial
meetings.  They ask for  your help in determining how they should analyze the data.

## 1

1. Data can be downloaded using

```{r}
juvenile <- read.csv ("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/juvenile_colonizer.csv", stringsAsFactors = T)
```

Help them analyze it correctly (maybe like you recommended in class, or not!). 
Make sure you include

* null hypothesis (1 pt)
* alternative hypothesis (1 pt)
* explanation for test/procedure you will use (1 pt)
* results from statistical test/procedure (1 pt)
* clear explanation of how results relate to your stated hypotheses (2 pt)

### Answer

Based on

Bueno M, Machado GBO, Leite FPP (2020) Colonization of novel algal habitats by 
juveniles of a marine tube-dwelling amphipod. PeerJ 8:e10188. https://doi.org/10.7717/peerj.10188

Data was in Supplement, focused on 
Presence_of_adults_lab tab.  I had to estimate raw numbers before 
uploading since proportion given and total ranged from 14-18; assumed 16 for all 
and multiplied by proportion (done in excel befoe uploading; also removed other 
odd info)

This is an example of a factorial-design, but the outcome is a proportion. For this
reason we will use a generalized linear model to analyze the data.  

* null hypothesis (1 pt)
  * H~O~: There is no impact on the habitat type juveniles start in on their 
  likelihood to move
  * H~O~: There is no impact on adult presence in the new habitat on the 
  likelihood of juveniles to move
  * H~O~:  There is no interaction between the impacts of the  habitat type 
  juveniles start in and  adult presence in the new habitat on the 
  likelihood of juveniles to move
* alternative hypothesis (1 pt)
  * H~A~: There is an impact on the habitat type juveniles start in on their 
  likelihood to move
  * H~A~: There is an impact on adult presence in the new habitat on the 
  likelihood of juveniles to move
  * H~A~:  There is an interaction between the impacts of the  habitat type 
  juveniles start in and  adult presence in the new habitat on the 
  likelihood of juveniles to move
* explanation for test you will use (1 pt)
  * As noted above, this is an example of a factorial-design, but the outcome is 
  a proportion. For thisreason we will use a generalized linear model to analyze the data. 
  
```{r}
str(juvenile)
```  
  
Let's make adults a factor  
  
```{r}
juvenile$adults <- factor(juvenile$adults)
```

and fit a glm using the binomial family.

```{r}
juvenile_fit_glm <- glm(cbind(Juveniles_colonized, Juveniles_not_colonized) ~ adults*source.habitat, 
    juvenile, family = "binomial")
```

Check assumptions
```{r}
plot(juvenile_fit_glm)
summary(juvenile_fit_glm)
```
Plots look fine. Dispersion is a little high (~90/30, so ~3), so you may want to
use a quasi-binomial, but not required

```{r}
library(car)
Anova(juvenile_fit_glm, type = "III")
```

Interaction is significant, so we should divide the data.  Before we do that, let's 
compare the various fits (not required for test)

```{r}
juvenile_fit_lm <- lm(Juveniles_colonized ~ adults*source.habitat, 
    juvenile)
juvenile_fit_quasi <- glm(cbind(Juveniles_colonized, Juveniles_not_colonized) ~ adults*source.habitat, 
    juvenile, family = "quasibinomial")
AIC(juvenile_fit_lm, juvenile_fit_glm, juvenile_fit_quasi)
```

Note we can't compare the quasi fit!  But in general

```{r}
summary(juvenile_fit_quasi)
Anova(juvenile_fit_quasi, type = "III")
```

Similar results. Showing outcome with binomial fit below for each habitat type

Artificial

```{r}
juvenile_fit_art <- glm(cbind(Juveniles_colonized, Juveniles_not_colonized) ~ adults, 
    juvenile[juvenile$source.habitat == "artificial",], family = "binomial")
plot(juvenile_fit_art)
summary(juvenile_fit_art)
Anova(juvenile_fit_art, type = "III")
```

```{r}
juvenile_fit_nat <- glm(cbind(Juveniles_colonized, Juveniles_not_colonized) ~ adults, 
    juvenile[juvenile$source.habitat == "natural",], family = "binomial")
plot(juvenile_fit_nat)
summary(juvenile_fit_nat)
Anova(juvenile_fit_nat, type = "III")
```

None

```{r}
juvenile_fit_none <- glm(cbind(Juveniles_colonized, Juveniles_not_colonized) ~ adults, 
    juvenile[juvenile$source.habitat == "none",], family = "binomial")
plot(juvenile_fit_none)
summary(juvenile_fit_none)
Anova(juvenile_fit_none, type = "III")
```

* results from statistical test (1 pt) and 
* clear explanation of how results relate to your stated hypotheses (2 pt)

We have a significant interaction between habitat type and adult presence 
($\chi$^2 = 27.8, p<.001).  Analysis of each habitat type indicates adults always
influence movement, but in different ways.  Adults reduce movement from artificial and none substrate but increase it from natural
To interpret the coefficients in odds ratios (or see them) note

```{r}
coef(juvenile_fit_art)[2]
exp(coef(juvenile_fit_art)[2])
coef(juvenile_fit_none)[2]
exp(coef(juvenile_fit_none)[2])
coef(juvenile_fit_nat)[2]
exp(coef(juvenile_fit_nat)[2])
```

A common question is comparing these. Though difficult with interactions, I would note the confidence intervals don't overlap (again, not required!).

## 2 

2. As a reminder/intro (from 78001 exam), your friend shared this graph with you
from the data:


```{r}
library(ggplot2)
ggplot(juvenile, aes(x=adults, y=Juveniles_colonized, fill = source.habitat)) +
         geom_col(size = 3)
```

Update the graph to better display the data.

### Answer

Lots of options here. We can make a proportion column (most common and what
auhors did):

![Graph from original paper](https://dfzljdn9uc3pi.cloudfront.net/2020/10188/1/fig-4-2x.jpg)

```{r}
juvenile$proportion <- juvenile$Juveniles_colonized/16

library(ggplot2)
ggplot(juvenile, aes(x=source.habitat, y=proportion, fill = adults)) +
  geom_boxplot()

library(Rmisc)
summary_juvenile <- summarySE(juvenile, measurevar = "proportion",
                              groupvars = c("adults", "source.habitat"))
ggplot(summary_juvenile, aes(x=source.habitat, y=proportion, fill = adults)) +
         geom_col(size = 3, position = "dodge") +
  geom_errorbar(aes(ymin=proportion-ci, ymax=proportion+ci), size=1.5, position = "dodge")
```

# Twitter

## 3


3. Following up on the class question where you designed a study to see if 
Twitter impacts citations - someone actually did this!

Authors collected Twitter activity (defined here to include three metrics: number 
of tweets, number of users, Twitter reach) and citation data on articles (from
Web of Science) 
from twenty journals that publish only ecological research. They randomly 
selected 
three articles from each issue of each journal.  They also have info
on when article was published, when twitter activty was collected (Collection.date),
and other columns (ask if needed!).   They selected
journals to represent a range of impact factors (identified from 
Thompson-Reuters 2014 Journal Citation Reports1 database). Data is available @ 

```{r}
twitter <- read.csv("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/S1Dataset.CSV")
str(twitter)
```

How would you analyze the data? If it helps, note you change the Collection and 
Publication Date columns

```{r}
twitter$Publication.date <- as.Date(twitter$Publication.date, format =c("%m/%d/%Y"))
twitter$Collection.date <- as.Date(twitter$Collection.date, format =c("%m/%d/%Y"))
```

and that any math done on those columns returnds difference in days by default.

As appropriate, include

* null hypothesis 
* alternative hypothesis 
* explanation for test/procedure you will use 
* results from statistical test/procedure
* clear explanation of how results relate to your stated hypotheses 

### Answer

Based on 

Peoples BK, Midway SR, Sackett D, et al (2016) Twitter Predicts Citation Rates of Ecological Research. PLOS ONE 11:e0166570. https://doi.org/10.1371/journal.pone.0166570

There are many ways to answer this. Below I try to recreate their approach and 
give general thoughts.

Most important, you need to note Journal is a random effect (or explain why you
only care about those 20!).  You also may want to control for how long an article
was live before data was collected.

```{r}
twitter$days_since_published <- twitter$Collection.date - twitter$Publication.date
head(twitter$days_since_published)
```

If you want to use a large model approach, you should probably consider correlation 
among variables. Their method

```{r}
library(ggplot2)
library(GGally)
twitter$days_since_published <- as.numeric(twitter$days_since_published)
ggpairs(twitter[,names(twitter) %in% c("Number.of.Web.of.Science.citations", 
                                       "X5.year.journal.impact.factor",
                                      "Number.of.tweets", "Number.of.users",
                                      "Twitter.reach", "days_since_published")],
        progress = F)

ggcorr(twitter[,names(twitter) %in% c("Number.of.Web.of.Science.citations", 
                                       "X5.year.journal.impact.factor",
                                      "Number.of.tweets", "Number.of.users",
                                      "Twitter.reach", "days_since_published")],
       palette = "RdBu", label = TRUE)

```

Number of users and Number of tweets is highly correlated, so just pick one (they
used number of tweets) and used *dredge* to consider all models.

```{r}
library(lme4)
twitter_fit_full <- lmer(Number.of.Web.of.Science.citations ~ 
                           Number.of.tweets + Twitter.reach + #twitter activity
                       X5.year.journal.impact.factor + days_since_published +  #things to control for
                     (1|Journal.identity), # random portion
                     twitter)
library(MuMIn)
options(na.action = "na.fail")
twitter_fit_full_output <- dredge(twitter_fit_full)
#model.avg for output. can decide how far delta can go. look at dredge output
head(twitter_fit_full_output)
#can average top models
model.avg(twitter_fit_full_output, subset = weight > 0) #using weight, which they used
#to get the top 1
top_model <- get.models(twitter_fit_full_output, subset = 1)[[1]]
top_model
```

We find a top model that only includes Number of tweets (in addition to days
since published and IF), but we also have strong support for including Twitter reach

We also get a scaling warning.  What if we scale (not done in class!)?

```{r}
twitter_scaled <- twitter
twitter_scaled[,names(twitter_scaled) %in% c( 
                                       "X5.year.journal.impact.factor",
                                      "Number.of.tweets", "Number.of.users",
                                      "Twitter.reach", "days_since_published")] <-
  scale(twitter_scaled[,names(twitter_scaled) %in% c( 
                                       "X5.year.journal.impact.factor",
                                      "Number.of.tweets", "Number.of.users",
                                      "Twitter.reach", "days_since_published")])
summary(twitter_scaled)
```

**Aside**: Notice scaling does not impact correlation! 

```{r}
ggpairs(twitter_scaled[,names(twitter_scaled) %in% c("Number.of.Web.of.Science.citations", 
                                       "X5.year.journal.impact.factor",
                                      "Number.of.tweets", "Number.of.users",
                                      "Twitter.reach", "days_since_published")],
        progress = F)
ggcorr(twitter_scaled[,names(twitter_scaled) %in% c("Number.of.Web.of.Science.citations", 
                                       "X5.year.journal.impact.factor",
                                      "Number.of.tweets", "Number.of.users",
                                      "Twitter.reach", "days_since_published")],
       palette = "RdBu", label = TRUE)
```

```{r}
twitter_scaled_fit_full <- lmer(Number.of.Web.of.Science.citations ~ 
                           Number.of.tweets + Twitter.reach + #twitter_scaled activity
                       X5.year.journal.impact.factor + days_since_published +  #things to control for
                     (1|Journal.identity), # random portion
                     twitter_scaled)
options(na.action = "na.fail")
auto <- dredge(twitter_scaled_fit_full)
write.csv(auto, "dredge_output_scaled.csv", row.names = F)
#model.avg for output. can decide how far delta can go. look at dredge output
head(auto)
#can average top models
model.avg(auto, subset = weight > 0) 
#to get the top 1
top_model <- get.models(auto, subset = 1)[[1]]
top_model
```

We get similar resuls (but not exactly what they found). Note they also considered
model importance (we did cover!), which compares the weight of each model that 
contains each variable

```{r}
sw(auto)
```

and effect sizes (not shown here, maybe add later!)

Other (simpler) options include just controlling for journal or other factors.
Some examples below.

```{r}
twitter_fit1 <- lmer(Number.of.Web.of.Science.citations ~ Number.of.tweets + 
                     (1|Journal.identity), twitter)
twitter_fit2 <- lmer(Number.of.Web.of.Science.citations ~ Number.of.tweets + 
                       X5.year.journal.impact.factor + days_since_published +
                       
                     (1|Journal.identity), twitter)
```

# Reef fish

As a reminder/intro (from class exam) - **Note change in parameters!**

>Your colleague returns to you with another reef question. This time they are
trying to see how length impacts weight in a fish species. They collected and 
measured fish. They bring you this plot 

```{r, echo=F}
bacalao <- read.csv("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/bacalao.csv", header = T)
ggplot(bacalao, aes(x=age_corr, y = TL))+
  geom_point()+
  labs(x="Corrected age", y  = "Total length (cm)")
```

> They also state they fit a “line R model” to the data. They made some graphs 
using code they found online..


```{r, echo=FALSE}
bacalao_fit_lm <- lm(TL~age_corr, bacalao)
par(mfrow=c(2,2))
plot(bacalao_fit_lm)
```

>And were very excited to get output using similarly borrowed code

```{r}
summary(bacalao_fit_lm)
```

>They are trying to write up the manuscript now, but are a little lost.  
Can you help them?

Anohter friend provided them with the following code

```{r}
bacalao_fit_nls <- nls(TL ~ Linf*(1-exp(-1*k*(age_corr-agezero))), bacalao, 
                       start = list(Linf = 110, k= .1, agezero = -1.7) )
```

which fits a von Bertannfy growth curve to the data.  Now they are even more 
confused.

## 4 

4. Explain (only words needed here) the difference among these models/functions.


## 5 

5. Can you compare the two approaches? Feel free to add another approach if you 
want (not required!).  Data can be imported using 

```{r}
bacalao <- read.csv("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/bacalao.csv", header = T)
```


### Answer and explanation

Based on

Usseglio P, Friedlander AM, DeMartini EE, et al (2015) Improved estimates of age, growth and reproduction for the regionally endemic Galapagos sailfin grouper Mycteroperca olfax (Jenyns, 1840). PeerJ 3:e1270. https://doi.org/10.7717/peerj.1270

I almost made you fit the curve, then decided against it.  The code I provided
fits the data using the 
von Bertallanfy equation, where

$$
L = L_\infty(1-e^{-k(t - t_O)}))
$$
L stands for length, L infinity is length at infinity,t stands for time since birth(age!) and time zero means length at birth.  
Parameters of interest can be found at 

![Figure from paper](https://dfzljdn9uc3pi.cloudfront.net/2015/1270/1/fig-4-2x.jpg), 

I would compare a linear and non-linear fit here AIC.

```{r}
AIC(bacalao_fit_lm, bacalao_fit_nls)
```

I could also add a generalized additive model approach.

```{r}
library(mgcv)
bacalao_fit_gam <- gam(TL~s(age_corr), data = bacalao)
summary(bacalao_fit_gam)
AIC(bacalao_fit_lm, bacalao_fit_nls, bacalao_fit_gam)
```
 You can also plot them
 
```{r}
ggplot(bacalao, aes(x=age_corr, y = TL))+
  geom_point()+
  labs(x="Corrected age", y  = "Total length (cm)") +
  geom_smooth(method = "lm", se = FALSE, size = 1.5, color = "orange")+ 
  geom_smooth(method="nls", 
              # look at whelk_power$call
              formula = y ~ Linf*(1-exp(-1*k*(x-agezero))), 
              method.args = list(start = list(Linf = 110, k= .1, agezero = -1.7)), 
              se=FALSE, size = 1.5, color = "blue") + 
  geom_smooth(stat= "smooth", method = "gam", formula = y ~ s(x), 
                         color = "yellow")
```
 


